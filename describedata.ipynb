{
 "cells": [
  {
   "cell_type": "raw",
   "id": "af94330b-1b6c-4975-8a1e-df9ae716852b",
   "metadata": {},
   "source": [
    "NETTOYAGE EXHAUSTIF DES DONNÉES - ÉLIMINATION DE L'OVERFITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf0bfa43-e76f-4a6e-8812-ade3c9d18990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DONNÉES ORIGINALES ===\n",
      "Shape initiale: (301, 9)\n",
      "Colonnes: ['Car_Name', 'Year', 'Selling_Price', 'Present_Price', 'Driven_kms', 'Fuel_Type', 'Selling_type', 'Transmission', 'Owner']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv(\"car data.csv\")\n",
    "print(\"=== DONNÉES ORIGINALES ===\")\n",
    "print(f\"Shape initiale: {df.shape}\")\n",
    "print(f\"Colonnes: {list(df.columns)}\")\n",
    "\n",
    "# Faire une copie pour le nettoyage\n",
    "df_clean = df.copy()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c50e26b-c37b-4ea6-a0e0-75ce2b5513e4",
   "metadata": {},
   "source": [
    "1. ANALYSE EXPLORATOIRE APPROFONDIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6f38bc7-de2c-4456-acf2-b63ab34afd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ANALYSE DE QUALITÉ ===\n",
      "Shape: (301, 9)\n",
      "Mémoire utilisée: 0.09 MB\n",
      "\n",
      "Valeurs manquantes:\n",
      "Empty DataFrame\n",
      "Columns: [Colonne, Valeurs_manquantes, Pourcentage]\n",
      "Index: []\n",
      "\n",
      "Doublons complets: 2\n",
      "\n",
      "Types de données:\n",
      "  Car_Name: object - Valeurs uniques: 98\n",
      "  Year: int64 - Valeurs uniques: 16\n",
      "  Selling_Price: float64 - Valeurs uniques: 156\n",
      "  Present_Price: float64 - Valeurs uniques: 148\n",
      "  Driven_kms: int64 - Valeurs uniques: 206\n",
      "  Fuel_Type: object - Valeurs uniques: 3\n",
      "  Selling_type: object - Valeurs uniques: 2\n",
      "  Transmission: object - Valeurs uniques: 2\n",
      "  Owner: int64 - Valeurs uniques: 3\n"
     ]
    }
   ],
   "source": [
    "def analyze_data_quality(df):\n",
    "    \"\"\"Analyse complète de la qualité des données\"\"\"\n",
    "    print(\"\\n=== ANALYSE DE QUALITÉ ===\")\n",
    "    \n",
    "    # Informations générales\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Mémoire utilisée: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Valeurs manquantes\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_percent = (missing_data / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Colonne': missing_data.index,\n",
    "        'Valeurs_manquantes': missing_data.values,\n",
    "        'Pourcentage': missing_percent.values\n",
    "    }).sort_values('Pourcentage', ascending=False)\n",
    "    \n",
    "    print(\"\\nValeurs manquantes:\")\n",
    "    print(missing_df[missing_df['Valeurs_manquantes'] > 0])\n",
    "    \n",
    "    # Doublons\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nDoublons complets: {duplicates}\")\n",
    "    \n",
    "    # Types de données\n",
    "    print(f\"\\nTypes de données:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"  {col}: {df[col].dtype} - Valeurs uniques: {df[col].nunique()}\")\n",
    "    \n",
    "    return missing_df\n",
    "\n",
    "# Analyse initiale\n",
    "quality_report = analyze_data_quality(df_clean)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "30929017-fa82-4b31-8084-a98ab2446aa4",
   "metadata": {},
   "source": [
    " 2. NETTOYAGE DES VALEURS ABERRANTES ET INCOHÉRENTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad6e41a4-744d-4dcd-89da-b222a05b6af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DÉTECTION DES OUTLIERS ===\n",
      "  Year: 7 outliers détectés (2.33%)\n",
      "  Selling_Price: 17 outliers détectés (5.65%)\n",
      "  Present_Price: 14 outliers détectés (4.65%)\n",
      "  Driven_kms: 8 outliers détectés (2.66%)\n",
      "  Owner: 11 outliers détectés (3.65%)\n"
     ]
    }
   ],
   "source": [
    "def clean_numerical_outliers(df, column, method='iqr', factor=1.5):\n",
    "    \"\"\"Nettoyer les outliers numériques\"\"\"\n",
    "    if method == 'iqr':\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - factor * IQR\n",
    "        upper_bound = Q3 + factor * IQR\n",
    "        \n",
    "        outliers_mask = (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "        \n",
    "    elif method == 'zscore':\n",
    "        z_scores = np.abs(stats.zscore(df[column].dropna()))\n",
    "        outliers_mask = z_scores > factor\n",
    "    \n",
    "    outliers_count = outliers_mask.sum()\n",
    "    print(f\"  {column}: {outliers_count} outliers détectés ({outliers_count/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    return outliers_mask\n",
    "\n",
    "# Identifier les colonnes numériques\n",
    "numeric_columns = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "target_col = 'Selling_Price'\n",
    "\n",
    "print(\"\\n=== DÉTECTION DES OUTLIERS ===\")\n",
    "outliers_masks = {}\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df_clean.columns:\n",
    "        # Exclure les valeurs négatives ou nulles pour les prix et années\n",
    "        if col in ['Selling_Price', 'Present_Price'] and (df_clean[col] <= 0).any():\n",
    "            print(f\"⚠️  {col}: Valeurs négatives/nulles détectées\")\n",
    "            df_clean = df_clean[df_clean[col] > 0]\n",
    "        \n",
    "        if col == 'Year' and (df_clean[col] < 1900).any():\n",
    "            print(f\"⚠️  {col}: Années suspectes détectées\")\n",
    "            df_clean = df_clean[df_clean[col] >= 1900]\n",
    "        \n",
    "        outliers_masks[col] = clean_numerical_outliers(df_clean, col)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "973e02f1-4426-4e55-926a-aa92c0dd9f5e",
   "metadata": {},
   "source": [
    "Supprimer les outliers extrêmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c486cce-26d4-4743-932f-7606696f3fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAITEMENT DES OUTLIERS ===\n",
      "  Year: 7 outliers supprimés (<=5%)\n",
      "  Selling_Price: 1 outliers extrêmes supprimés (>5%)\n",
      "  Present_Price: 14 outliers supprimés (<=5%)\n",
      "  Driven_kms: 8 outliers supprimés (<=5%)\n",
      "  Owner: 11 outliers supprimés (<=5%)\n",
      "\n",
      "Shape après nettoyage des outliers : (268, 9)\n"
     ]
    }
   ],
   "source": [
    "def remove_extreme_outliers(df, outliers_masks, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Supprime les outliers extrêmes pour chaque colonne numé­rique,\n",
    "    en étant plus tolérant si la proportion dépasse un certain seuil.\n",
    "\n",
    "    Paramètres :\n",
    "    - df : DataFrame d'origine\n",
    "    - outliers_masks : dictionnaire {colonne: masque booléen des outliers}\n",
    "    - threshold : seuil de tolérance (par défaut 5%) pour décider d'être plus conservateur\n",
    "\n",
    "    Retour :\n",
    "    - df nettoyé des outliers extrêmes\n",
    "    \"\"\"\n",
    "    df_no_outliers = df.copy()\n",
    "    \n",
    "    for col, mask in outliers_masks.items():\n",
    "        proportion_outliers = mask.sum() / len(df)\n",
    "        if proportion_outliers > threshold:\n",
    "            # Conserver uniquement les 5% des outliers les plus extrêmes\n",
    "            outlier_values = df[mask][col]\n",
    "            extreme_threshold = np.percentile(outlier_values, 95)\n",
    "            extreme_mask = mask & (df[col] > extreme_threshold)\n",
    "            df_no_outliers = df_no_outliers[~extreme_mask]\n",
    "            print(f\"  {col}: {extreme_mask.sum()} outliers extrêmes supprimés (>{threshold*100:.0f}%)\")\n",
    "        else:\n",
    "            df_no_outliers = df_no_outliers[~mask]\n",
    "            print(f\"  {col}: {mask.sum()} outliers supprimés (<={threshold*100:.0f}%)\")\n",
    "    \n",
    "    return df_no_outliers\n",
    "\n",
    "\n",
    "# ===== Exécution du nettoyage des outliers =====\n",
    "print(\"\\n=== TRAITEMENT DES OUTLIERS ===\")\n",
    "df_clean = remove_extreme_outliers(df_clean, outliers_masks)\n",
    "\n",
    "print(f\"\\nShape après nettoyage des outliers : {df_clean.shape}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e033c35b-ad18-4739-a0dc-fb7b3361c6a7",
   "metadata": {},
   "source": [
    "3. NETTOYAGE DES DONNÉES CATÉGORIELLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c4a50e2-0053-4f47-b80a-519104646f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NETTOYAGE CATÉGORIEL ===\n",
      "\n",
      "Car_Name:\n",
      "  Valeurs uniques avant: 89\n",
      "  58 catégories rares regroupées\n",
      "  Valeurs uniques après: 32\n",
      "  Top 5 valeurs: {'other_rare': 75, 'city': 26, 'corolla altis': 16, 'verna': 14, 'brio': 10}\n",
      "\n",
      "Fuel_Type:\n",
      "  Valeurs uniques avant: 3\n",
      "  1 catégories rares regroupées\n",
      "  Valeurs uniques après: 3\n",
      "  Top 5 valeurs: {'petrol': 220, 'diesel': 46, 'other_rare': 2}\n",
      "\n",
      "Selling_type:\n",
      "  Valeurs uniques avant: 2\n",
      "  Valeurs uniques après: 2\n",
      "  Top 5 valeurs: {'dealer': 175, 'individual': 93}\n",
      "\n",
      "Transmission:\n",
      "  Valeurs uniques avant: 2\n",
      "  Valeurs uniques après: 2\n",
      "  Top 5 valeurs: {'manual': 243, 'automatic': 25}\n"
     ]
    }
   ],
   "source": [
    "def clean_categorical_data(df):\n",
    "    \"\"\"Nettoyer les données catégorielles\"\"\"\n",
    "    df_cat_clean = df.copy()\n",
    "    \n",
    "    print(\"\\n=== NETTOYAGE CATÉGORIEL ===\")\n",
    "    \n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if col in df_cat_clean.columns:\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  Valeurs uniques avant: {df_cat_clean[col].nunique()}\")\n",
    "            \n",
    "            # Nettoyer les espaces et la casse\n",
    "            df_cat_clean[col] = df_cat_clean[col].astype(str).str.strip().str.lower()\n",
    "            \n",
    "            # Remplacer les valeurs communes\n",
    "            df_cat_clean[col] = df_cat_clean[col].replace({\n",
    "                'nan': np.nan,\n",
    "                'unknown': np.nan,\n",
    "                'other': np.nan,\n",
    "                '': np.nan\n",
    "            })\n",
    "            \n",
    "            # Regrouper les catégories rares (moins de 1% des données)\n",
    "            value_counts = df_cat_clean[col].value_counts()\n",
    "            rare_categories = value_counts[value_counts < len(df_cat_clean) * 0.01].index\n",
    "            \n",
    "            if len(rare_categories) > 0:\n",
    "                df_cat_clean[col] = df_cat_clean[col].replace(rare_categories, 'other_rare')\n",
    "                print(f\"  {len(rare_categories)} catégories rares regroupées\")\n",
    "            \n",
    "            print(f\"  Valeurs uniques après: {df_cat_clean[col].nunique()}\")\n",
    "            print(f\"  Top 5 valeurs: {df_cat_clean[col].value_counts().head().to_dict()}\")\n",
    "    \n",
    "    return df_cat_clean\n",
    "\n",
    "df_clean = clean_categorical_data(df_clean)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c60f526-0c52-49dd-8d36-3d608f9439ba",
   "metadata": {},
   "source": [
    "4. FEATURE ENGINEERING AVANCÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7412c114-5df5-46fc-9602-63f5873efcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FEATURE ENGINEERING ===\n",
      "✅ Car_Age créé\n",
      "✅ Depreciation_Ratio créé\n",
      "✅ Age_Category créé\n",
      "✅ Selling_Price_log créé\n",
      "✅ Present_Price_log créé\n"
     ]
    }
   ],
   "source": [
    "def create_engineered_features(df):\n",
    "    \"\"\"Créer des features engineered pour réduire l'overfitting\"\"\"\n",
    "    df_eng = df.copy()\n",
    "    \n",
    "    print(\"\\n=== FEATURE ENGINEERING ===\")\n",
    "    \n",
    "    # Age de la voiture (plus informatif que l'année seule)\n",
    "    if 'Year' in df_eng.columns:\n",
    "        current_year = 2024  # Ajustez selon vos données\n",
    "        df_eng['Car_Age'] = current_year - df_eng['Year']\n",
    "        print(\"✅ Car_Age créé\")\n",
    "    \n",
    "    # Ratio de dépréciation\n",
    "    if 'Present_Price' in df_eng.columns and 'Selling_Price' in df_eng.columns:\n",
    "        df_eng['Depreciation_Ratio'] = (df_eng['Present_Price'] - df_eng['Selling_Price']) / df_eng['Present_Price']\n",
    "        df_eng['Depreciation_Ratio'] = df_eng['Depreciation_Ratio'].clip(0, 1)  # Limiter entre 0 et 1\n",
    "        print(\"✅ Depreciation_Ratio créé\")\n",
    "    \n",
    "    # Binning de l'âge en catégories\n",
    "    if 'Car_Age' in df_eng.columns:\n",
    "        df_eng['Age_Category'] = pd.cut(df_eng['Car_Age'], \n",
    "                                       bins=[0, 2, 5, 10, float('inf')], \n",
    "                                       labels=['Nouveau', 'Recent', 'Moyen', 'Ancien'])\n",
    "        print(\"✅ Age_Category créé\")\n",
    "    \n",
    "    # Log transformation pour les prix (réduire la variance)\n",
    "    price_columns = ['Selling_Price', 'Present_Price']\n",
    "    for col in price_columns:\n",
    "        if col in df_eng.columns:\n",
    "            df_eng[f'{col}_log'] = np.log1p(df_eng[col])\n",
    "            print(f\"✅ {col}_log créé\")\n",
    "    \n",
    "    return df_eng\n",
    "\n",
    "df_clean = create_engineered_features(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a80337-68cc-456b-8fd3-8fedc80f6bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. DÉTECTION ET SUPPRESSION DES DOUBLONS SOPHISTIQUÉS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "137a8ec4-41c9-4ad7-9812-ad28f011b750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SUPPRESSION DOUBLONS SOPHISTIQUÉS ===\n",
      "Doublons exacts supprimés: 1\n",
      "Doublons par colonnes clés supprimés: 44\n"
     ]
    }
   ],
   "source": [
    "def remove_sophisticated_duplicates(df):\n",
    "    \"\"\"Supprimer les doublons sophistiqués\"\"\"\n",
    "    df_dedup = df.copy()\n",
    "    \n",
    "    print(\"\\n=== SUPPRESSION DOUBLONS SOPHISTIQUÉS ===\")\n",
    "    \n",
    "    # Doublons exacts\n",
    "    exact_dups = df_dedup.duplicated().sum()\n",
    "    df_dedup = df_dedup.drop_duplicates()\n",
    "    print(f\"Doublons exacts supprimés: {exact_dups}\")\n",
    "    \n",
    "    # Doublons basés sur des colonnes clés (exemple)\n",
    "    key_columns = ['Year', 'Present_Price', 'Fuel_Type', 'Transmission']\n",
    "    if all(col in df_dedup.columns for col in key_columns):\n",
    "        before_count = len(df_dedup)\n",
    "        df_dedup = df_dedup.drop_duplicates(subset=key_columns)\n",
    "        key_dups = before_count - len(df_dedup)\n",
    "        print(f\"Doublons par colonnes clés supprimés: {key_dups}\")\n",
    "    \n",
    "    return df_dedup\n",
    "\n",
    "df_clean = remove_sophisticated_duplicates(df_clean)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ec4c39d-a859-4c01-b266-2cb11eed60f1",
   "metadata": {},
   "source": [
    "6. VALIDATION ET FILTRAGE DE COHÉRENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b67e69d-67e6-4427-ba42-aed8d898faab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VALIDATION DE COHÉRENCE ===\n",
      "Lignes avec prix de vente > 120% prix présent supprimées: 0\n",
      "Voitures avec année future supprimées: 0\n",
      "Lignes avec Driven_kms négatif supprimées: 0\n",
      "Total lignes supprimées pour incohérence: 0\n"
     ]
    }
   ],
   "source": [
    "def validate_data_consistency(df):\n",
    "    \"\"\"Valider la cohérence des données\"\"\"\n",
    "    df_valid = df.copy()\n",
    "    \n",
    "    print(\"\\n=== VALIDATION DE COHÉRENCE ===\")\n",
    "    \n",
    "    initial_count = len(df_valid)\n",
    "    \n",
    "    # Selling_Price ne peut pas être supérieur à Present_Price (dans la plupart des cas)\n",
    "    if 'Selling_Price' in df_valid.columns and 'Present_Price' in df_valid.columns:\n",
    "        inconsistent = df_valid['Selling_Price'] > df_valid['Present_Price'] * 1.2  # 20% de marge\n",
    "        df_valid = df_valid[~inconsistent]\n",
    "        print(f\"Lignes avec prix de vente > 120% prix présent supprimées: {inconsistent.sum()}\")\n",
    "    \n",
    "    # Années futures\n",
    "    if 'Year' in df_valid.columns:\n",
    "        future_years = df_valid['Year'] > 2024\n",
    "        df_valid = df_valid[~future_years]\n",
    "        print(f\"Voitures avec année future supprimées: {future_years.sum()}\")\n",
    "    \n",
    "    # Kilométrage négatif (si cette colonne existe)\n",
    "    km_columns = [col for col in df_valid.columns if 'km' in col.lower() or 'mileage' in col.lower()]\n",
    "    for col in km_columns:\n",
    "        negative_km = df_valid[col] < 0\n",
    "        df_valid = df_valid[~negative_km]\n",
    "        print(f\"Lignes avec {col} négatif supprimées: {negative_km.sum()}\")\n",
    "    \n",
    "    final_count = len(df_valid)\n",
    "    print(f\"Total lignes supprimées pour incohérence: {initial_count - final_count}\")\n",
    "    \n",
    "    return df_valid\n",
    "\n",
    "df_clean = validate_data_consistency(df_clean)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39d6dd06-145d-4869-b0e2-7c7802eee573",
   "metadata": {},
   "source": [
    "7. NETTOYAGE FINAL ET RAPPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ab0506c-2ef5-4e50-8181-b97978693d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RAPPORT DE NETTOYAGE FINAL\n",
      "============================================================\n",
      "Shape originale: (301, 9)\n",
      "Shape nettoyée: (223, 14)\n",
      "Lignes supprimées: 78 (25.9%)\n",
      "\n",
      "✅ Aucune valeur manquante restante!\n",
      "\n",
      "Statistiques Selling_Price:\n",
      "  Moyenne: 3.90\n",
      "  Médiane: 3.35\n",
      "  Écart-type: 3.42\n",
      "  Min-Max: 0.10 - 18.00\n",
      "\n",
      "Features finales (14):\n",
      "  Age_Category: category (2 valeurs uniques)\n",
      "  Car_Age: int64 (13 valeurs uniques)\n",
      "  Car_Name: object (32 valeurs uniques)\n",
      "  Depreciation_Ratio: float64 (213 valeurs uniques)\n",
      "  Driven_kms: int64 (166 valeurs uniques)\n",
      "  Fuel_Type: object (3 valeurs uniques)\n",
      "  Owner: int64 (1 valeurs uniques)\n",
      "  Present_Price: float64 (135 valeurs uniques)\n",
      "  Present_Price_log: float64 (135 valeurs uniques)\n",
      "  Selling_Price: float64 (121 valeurs uniques)\n",
      "  Selling_Price_log: float64 (121 valeurs uniques)\n",
      "  Selling_type: object (2 valeurs uniques)\n",
      "  Transmission: object (2 valeurs uniques)\n",
      "  Year: int64 (13 valeurs uniques)\n"
     ]
    }
   ],
   "source": [
    "def final_cleanup_and_report(df_original, df_clean):\n",
    "    \"\"\"Nettoyage final et rapport de synthèse\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RAPPORT DE NETTOYAGE FINAL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Shape originale: {df_original.shape}\")\n",
    "    print(f\"Shape nettoyée: {df_clean.shape}\")\n",
    "    print(f\"Lignes supprimées: {len(df_original) - len(df_clean)} ({(len(df_original) - len(df_clean))/len(df_original)*100:.1f}%)\")\n",
    "    \n",
    "    # Valeurs manquantes restantes\n",
    "    missing_final = df_clean.isnull().sum()\n",
    "    if missing_final.sum() > 0:\n",
    "        print(f\"\\nValeurs manquantes restantes:\")\n",
    "        for col, count in missing_final[missing_final > 0].items():\n",
    "            print(f\"  {col}: {count} ({count/len(df_clean)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"\\n✅ Aucune valeur manquante restante!\")\n",
    "    \n",
    "    # Distribution de la variable cible\n",
    "    if 'Selling_Price' in df_clean.columns:\n",
    "        target_stats = df_clean['Selling_Price'].describe()\n",
    "        print(f\"\\nStatistiques Selling_Price:\")\n",
    "        print(f\"  Moyenne: {target_stats['mean']:.2f}\")\n",
    "        print(f\"  Médiane: {target_stats['50%']:.2f}\")\n",
    "        print(f\"  Écart-type: {target_stats['std']:.2f}\")\n",
    "        print(f\"  Min-Max: {target_stats['min']:.2f} - {target_stats['max']:.2f}\")\n",
    "    \n",
    "    # Features finales\n",
    "    print(f\"\\nFeatures finales ({len(df_clean.columns)}):\")\n",
    "    for col in sorted(df_clean.columns):\n",
    "        dtype = df_clean[col].dtype\n",
    "        nunique = df_clean[col].nunique()\n",
    "        print(f\"  {col}: {dtype} ({nunique} valeurs uniques)\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "df_final = final_cleanup_and_report(df, df_clean)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61ea19e6-f3c5-41a1-8716-09e8b3f2cf5f",
   "metadata": {},
   "source": [
    "8. SAUVEGARDE DES DONNÉES NETTOYÉES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2836d23-981a-4591-8439-7c01a6e937d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Données nettoyées sauvegardées dans 'car_data_cleaned.csv'\n",
      "\n",
      "=== UTILISEZ MAINTENANT CES DONNÉES POUR L'ENTRAÎNEMENT ===\n",
      "# Pour charger les données nettoyées:\n",
      "df_clean = pd.read_csv('car_data_cleaned.csv')\n",
      "# Procédez avec votre pipeline de ML habituel\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarder les données nettoyées\n",
    "df_final.to_csv('car_data_cleaned.csv', index=False)\n",
    "print(f\"\\n✅ Données nettoyées sauvegardées dans 'car_data_cleaned.csv'\")\n",
    "\n",
    "# Créer un rapport de nettoyage\n",
    "cleaning_report = {\n",
    "    'original_shape': df.shape,\n",
    "    'final_shape': df_final.shape,\n",
    "    'rows_removed': len(df) - len(df_final),\n",
    "    'percentage_removed': (len(df) - len(df_final)) / len(df) * 100,\n",
    "    'columns_added': len(df_final.columns) - len(df.columns)\n",
    "}\n",
    "\n",
    "print(f\"\\n=== UTILISEZ MAINTENANT CES DONNÉES POUR L'ENTRAÎNEMENT ===\")\n",
    "print(\"# Pour charger les données nettoyées:\")\n",
    "print(\"df_clean = pd.read_csv('car_data_cleaned.csv')\")\n",
    "print(\"# Procédez avec votre pipeline de ML habituel\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6dfbc512-7f73-4c8f-80e3-0166f5599b95",
   "metadata": {},
   "source": [
    "9. VISUALISATIONS OPTIONNELLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "993d67ae-d041-4da6-ba83-f4f14b3cb3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎉 NETTOYAGE TERMINÉ ! Vos données sont maintenant prêtes pour un entraînement sans overfitting.\n"
     ]
    }
   ],
   "source": [
    "def create_cleaning_visualizations(df_original, df_clean):\n",
    "    \"\"\"Créer des visualisations pour montrer l'impact du nettoyage\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Distribution du prix avant/après\n",
    "    if 'Selling_Price' in df_original.columns:\n",
    "        axes[0,0].hist(df_original['Selling_Price'], bins=50, alpha=0.7, label='Original', color='red')\n",
    "        axes[0,0].hist(df_clean['Selling_Price'], bins=50, alpha=0.7, label='Nettoyé', color='blue')\n",
    "        axes[0,0].set_title('Distribution Selling_Price')\n",
    "        axes[0,0].legend()\n",
    "        axes[0,0].set_xlabel('Prix')\n",
    "        axes[0,0].set_ylabel('Fréquence')\n",
    "    \n",
    "    # Boxplot comparatif\n",
    "    if 'Selling_Price' in df_original.columns:\n",
    "        data_comparison = [df_original['Selling_Price'].dropna(), df_clean['Selling_Price'].dropna()]\n",
    "        axes[0,1].boxplot(data_comparison, labels=['Original', 'Nettoyé'])\n",
    "        axes[0,1].set_title('Boxplot Selling_Price')\n",
    "        axes[0,1].set_ylabel('Prix')\n",
    "    \n",
    "    # Corrélations avant/après (si suffisamment de colonnes numériques)\n",
    "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 2:\n",
    "        corr_clean = df_clean[numeric_cols].corr()\n",
    "        im = axes[1,0].imshow(corr_clean, cmap='coolwarm', aspect='auto')\n",
    "        axes[1,0].set_title('Matrice de corrélation (données nettoyées)')\n",
    "        plt.colorbar(im, ax=axes[1,0])\n",
    "    \n",
    "    # Valeurs manquantes avant/après\n",
    "    missing_orig = df_original.isnull().sum()\n",
    "    missing_clean = df_clean.isnull().sum()\n",
    "    \n",
    "    x_pos = range(len(missing_orig))\n",
    "    axes[1,1].bar([x-0.2 for x in x_pos], missing_orig.values, width=0.4, label='Original', alpha=0.7)\n",
    "    axes[1,1].bar([x+0.2 for x in x_pos], missing_clean.values, width=0.4, label='Nettoyé', alpha=0.7)\n",
    "    axes[1,1].set_title('Valeurs manquantes par colonne')\n",
    "    axes[1,1].set_xticks(x_pos)\n",
    "    axes[1,1].set_xticklabels(missing_orig.index, rotation=45)\n",
    "    axes[1,1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('data_cleaning_report.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"📊 Graphiques de nettoyage sauvegardés dans 'data_cleaning_report.png'\")\n",
    "\n",
    "# Créer les visualisations (optionnel)\n",
    "# create_cleaning_visualizations(df, df_final)\n",
    "\n",
    "print(\"\\n🎉 NETTOYAGE TERMINÉ ! Vos données sont maintenant prêtes pour un entraînement sans overfitting.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9bc07ccc-e25d-45bc-9696-2352e5b94ddd",
   "metadata": {},
   "source": [
    "Remark:\n",
    "Given that the dataset contains only 301 samples and we have engineered 3 additional features, there is a significant risk of overfitting. With a relatively high feature-to-sample ratio, the model might capture noise and specific patterns in the training data that do not generalize well to unseen data. To mitigate this risk, it is crucial to perform proper feature selection, apply regularization techniques, and use rigorous cross-validation strategies to ensure the model’s robustness and generalization capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f9e71-7878-47a7-9755-b81c309d9a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
