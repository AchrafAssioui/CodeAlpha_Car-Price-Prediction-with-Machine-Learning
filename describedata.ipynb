{
 "cells": [
  {
   "cell_type": "raw",
   "id": "af94330b-1b6c-4975-8a1e-df9ae716852b",
   "metadata": {},
   "source": [
    "NETTOYAGE EXHAUSTIF DES DONN√âES - √âLIMINATION DE L'OVERFITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf0bfa43-e76f-4a6e-8812-ade3c9d18990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DONN√âES ORIGINALES ===\n",
      "Shape initiale: (301, 9)\n",
      "Colonnes: ['Car_Name', 'Year', 'Selling_Price', 'Present_Price', 'Driven_kms', 'Fuel_Type', 'Selling_type', 'Transmission', 'Owner']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Charger les donn√©es\n",
    "df = pd.read_csv(\"car data.csv\")\n",
    "print(\"=== DONN√âES ORIGINALES ===\")\n",
    "print(f\"Shape initiale: {df.shape}\")\n",
    "print(f\"Colonnes: {list(df.columns)}\")\n",
    "\n",
    "# Faire une copie pour le nettoyage\n",
    "df_clean = df.copy()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c50e26b-c37b-4ea6-a0e0-75ce2b5513e4",
   "metadata": {},
   "source": [
    "1. ANALYSE EXPLORATOIRE APPROFONDIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6f38bc7-de2c-4456-acf2-b63ab34afd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ANALYSE DE QUALIT√â ===\n",
      "Shape: (301, 9)\n",
      "M√©moire utilis√©e: 0.09 MB\n",
      "\n",
      "Valeurs manquantes:\n",
      "Empty DataFrame\n",
      "Columns: [Colonne, Valeurs_manquantes, Pourcentage]\n",
      "Index: []\n",
      "\n",
      "Doublons complets: 2\n",
      "\n",
      "Types de donn√©es:\n",
      "  Car_Name: object - Valeurs uniques: 98\n",
      "  Year: int64 - Valeurs uniques: 16\n",
      "  Selling_Price: float64 - Valeurs uniques: 156\n",
      "  Present_Price: float64 - Valeurs uniques: 148\n",
      "  Driven_kms: int64 - Valeurs uniques: 206\n",
      "  Fuel_Type: object - Valeurs uniques: 3\n",
      "  Selling_type: object - Valeurs uniques: 2\n",
      "  Transmission: object - Valeurs uniques: 2\n",
      "  Owner: int64 - Valeurs uniques: 3\n"
     ]
    }
   ],
   "source": [
    "def analyze_data_quality(df):\n",
    "    \"\"\"Analyse compl√®te de la qualit√© des donn√©es\"\"\"\n",
    "    print(\"\\n=== ANALYSE DE QUALIT√â ===\")\n",
    "    \n",
    "    # Informations g√©n√©rales\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"M√©moire utilis√©e: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Valeurs manquantes\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_percent = (missing_data / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Colonne': missing_data.index,\n",
    "        'Valeurs_manquantes': missing_data.values,\n",
    "        'Pourcentage': missing_percent.values\n",
    "    }).sort_values('Pourcentage', ascending=False)\n",
    "    \n",
    "    print(\"\\nValeurs manquantes:\")\n",
    "    print(missing_df[missing_df['Valeurs_manquantes'] > 0])\n",
    "    \n",
    "    # Doublons\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nDoublons complets: {duplicates}\")\n",
    "    \n",
    "    # Types de donn√©es\n",
    "    print(f\"\\nTypes de donn√©es:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"  {col}: {df[col].dtype} - Valeurs uniques: {df[col].nunique()}\")\n",
    "    \n",
    "    return missing_df\n",
    "\n",
    "# Analyse initiale\n",
    "quality_report = analyze_data_quality(df_clean)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "30929017-fa82-4b31-8084-a98ab2446aa4",
   "metadata": {},
   "source": [
    " 2. NETTOYAGE DES VALEURS ABERRANTES ET INCOH√âRENTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad6e41a4-744d-4dcd-89da-b222a05b6af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== D√âTECTION DES OUTLIERS ===\n",
      "  Year: 7 outliers d√©tect√©s (2.33%)\n",
      "  Selling_Price: 17 outliers d√©tect√©s (5.65%)\n",
      "  Present_Price: 14 outliers d√©tect√©s (4.65%)\n",
      "  Driven_kms: 8 outliers d√©tect√©s (2.66%)\n",
      "  Owner: 11 outliers d√©tect√©s (3.65%)\n"
     ]
    }
   ],
   "source": [
    "def clean_numerical_outliers(df, column, method='iqr', factor=1.5):\n",
    "    \"\"\"Nettoyer les outliers num√©riques\"\"\"\n",
    "    if method == 'iqr':\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - factor * IQR\n",
    "        upper_bound = Q3 + factor * IQR\n",
    "        \n",
    "        outliers_mask = (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "        \n",
    "    elif method == 'zscore':\n",
    "        z_scores = np.abs(stats.zscore(df[column].dropna()))\n",
    "        outliers_mask = z_scores > factor\n",
    "    \n",
    "    outliers_count = outliers_mask.sum()\n",
    "    print(f\"  {column}: {outliers_count} outliers d√©tect√©s ({outliers_count/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    return outliers_mask\n",
    "\n",
    "# Identifier les colonnes num√©riques\n",
    "numeric_columns = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "target_col = 'Selling_Price'\n",
    "\n",
    "print(\"\\n=== D√âTECTION DES OUTLIERS ===\")\n",
    "outliers_masks = {}\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df_clean.columns:\n",
    "        # Exclure les valeurs n√©gatives ou nulles pour les prix et ann√©es\n",
    "        if col in ['Selling_Price', 'Present_Price'] and (df_clean[col] <= 0).any():\n",
    "            print(f\"‚ö†Ô∏è  {col}: Valeurs n√©gatives/nulles d√©tect√©es\")\n",
    "            df_clean = df_clean[df_clean[col] > 0]\n",
    "        \n",
    "        if col == 'Year' and (df_clean[col] < 1900).any():\n",
    "            print(f\"‚ö†Ô∏è  {col}: Ann√©es suspectes d√©tect√©es\")\n",
    "            df_clean = df_clean[df_clean[col] >= 1900]\n",
    "        \n",
    "        outliers_masks[col] = clean_numerical_outliers(df_clean, col)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "973e02f1-4426-4e55-926a-aa92c0dd9f5e",
   "metadata": {},
   "source": [
    "Supprimer les outliers extr√™mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c486cce-26d4-4743-932f-7606696f3fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAITEMENT DES OUTLIERS ===\n",
      "  Year: 7 outliers supprim√©s (<=5%)\n",
      "  Selling_Price: 1 outliers extr√™mes supprim√©s (>5%)\n",
      "  Present_Price: 14 outliers supprim√©s (<=5%)\n",
      "  Driven_kms: 8 outliers supprim√©s (<=5%)\n",
      "  Owner: 11 outliers supprim√©s (<=5%)\n",
      "\n",
      "Shape apr√®s nettoyage des outliers : (268, 9)\n"
     ]
    }
   ],
   "source": [
    "def remove_extreme_outliers(df, outliers_masks, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Supprime les outliers extr√™mes pour chaque colonne num√©¬≠rique,\n",
    "    en √©tant plus tol√©rant si la proportion d√©passe un certain seuil.\n",
    "\n",
    "    Param√®tres :\n",
    "    - df : DataFrame d'origine\n",
    "    - outliers_masks : dictionnaire {colonne: masque bool√©en des outliers}\n",
    "    - threshold : seuil de tol√©rance (par d√©faut 5%) pour d√©cider d'√™tre plus conservateur\n",
    "\n",
    "    Retour :\n",
    "    - df nettoy√© des outliers extr√™mes\n",
    "    \"\"\"\n",
    "    df_no_outliers = df.copy()\n",
    "    \n",
    "    for col, mask in outliers_masks.items():\n",
    "        proportion_outliers = mask.sum() / len(df)\n",
    "        if proportion_outliers > threshold:\n",
    "            # Conserver uniquement les 5% des outliers les plus extr√™mes\n",
    "            outlier_values = df[mask][col]\n",
    "            extreme_threshold = np.percentile(outlier_values, 95)\n",
    "            extreme_mask = mask & (df[col] > extreme_threshold)\n",
    "            df_no_outliers = df_no_outliers[~extreme_mask]\n",
    "            print(f\"  {col}: {extreme_mask.sum()} outliers extr√™mes supprim√©s (>{threshold*100:.0f}%)\")\n",
    "        else:\n",
    "            df_no_outliers = df_no_outliers[~mask]\n",
    "            print(f\"  {col}: {mask.sum()} outliers supprim√©s (<={threshold*100:.0f}%)\")\n",
    "    \n",
    "    return df_no_outliers\n",
    "\n",
    "\n",
    "# ===== Ex√©cution du nettoyage des outliers =====\n",
    "print(\"\\n=== TRAITEMENT DES OUTLIERS ===\")\n",
    "df_clean = remove_extreme_outliers(df_clean, outliers_masks)\n",
    "\n",
    "print(f\"\\nShape apr√®s nettoyage des outliers : {df_clean.shape}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e033c35b-ad18-4739-a0dc-fb7b3361c6a7",
   "metadata": {},
   "source": [
    "3. NETTOYAGE DES DONN√âES CAT√âGORIELLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c4a50e2-0053-4f47-b80a-519104646f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NETTOYAGE CAT√âGORIEL ===\n",
      "\n",
      "Car_Name:\n",
      "  Valeurs uniques avant: 89\n",
      "  58 cat√©gories rares regroup√©es\n",
      "  Valeurs uniques apr√®s: 32\n",
      "  Top 5 valeurs: {'other_rare': 75, 'city': 26, 'corolla altis': 16, 'verna': 14, 'brio': 10}\n",
      "\n",
      "Fuel_Type:\n",
      "  Valeurs uniques avant: 3\n",
      "  1 cat√©gories rares regroup√©es\n",
      "  Valeurs uniques apr√®s: 3\n",
      "  Top 5 valeurs: {'petrol': 220, 'diesel': 46, 'other_rare': 2}\n",
      "\n",
      "Selling_type:\n",
      "  Valeurs uniques avant: 2\n",
      "  Valeurs uniques apr√®s: 2\n",
      "  Top 5 valeurs: {'dealer': 175, 'individual': 93}\n",
      "\n",
      "Transmission:\n",
      "  Valeurs uniques avant: 2\n",
      "  Valeurs uniques apr√®s: 2\n",
      "  Top 5 valeurs: {'manual': 243, 'automatic': 25}\n"
     ]
    }
   ],
   "source": [
    "def clean_categorical_data(df):\n",
    "    \"\"\"Nettoyer les donn√©es cat√©gorielles\"\"\"\n",
    "    df_cat_clean = df.copy()\n",
    "    \n",
    "    print(\"\\n=== NETTOYAGE CAT√âGORIEL ===\")\n",
    "    \n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if col in df_cat_clean.columns:\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  Valeurs uniques avant: {df_cat_clean[col].nunique()}\")\n",
    "            \n",
    "            # Nettoyer les espaces et la casse\n",
    "            df_cat_clean[col] = df_cat_clean[col].astype(str).str.strip().str.lower()\n",
    "            \n",
    "            # Remplacer les valeurs communes\n",
    "            df_cat_clean[col] = df_cat_clean[col].replace({\n",
    "                'nan': np.nan,\n",
    "                'unknown': np.nan,\n",
    "                'other': np.nan,\n",
    "                '': np.nan\n",
    "            })\n",
    "            \n",
    "            # Regrouper les cat√©gories rares (moins de 1% des donn√©es)\n",
    "            value_counts = df_cat_clean[col].value_counts()\n",
    "            rare_categories = value_counts[value_counts < len(df_cat_clean) * 0.01].index\n",
    "            \n",
    "            if len(rare_categories) > 0:\n",
    "                df_cat_clean[col] = df_cat_clean[col].replace(rare_categories, 'other_rare')\n",
    "                print(f\"  {len(rare_categories)} cat√©gories rares regroup√©es\")\n",
    "            \n",
    "            print(f\"  Valeurs uniques apr√®s: {df_cat_clean[col].nunique()}\")\n",
    "            print(f\"  Top 5 valeurs: {df_cat_clean[col].value_counts().head().to_dict()}\")\n",
    "    \n",
    "    return df_cat_clean\n",
    "\n",
    "df_clean = clean_categorical_data(df_clean)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c60f526-0c52-49dd-8d36-3d608f9439ba",
   "metadata": {},
   "source": [
    "4. FEATURE ENGINEERING AVANC√â"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7412c114-5df5-46fc-9602-63f5873efcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FEATURE ENGINEERING ===\n",
      "‚úÖ Car_Age cr√©√©\n",
      "‚úÖ Depreciation_Ratio cr√©√©\n",
      "‚úÖ Age_Category cr√©√©\n",
      "‚úÖ Selling_Price_log cr√©√©\n",
      "‚úÖ Present_Price_log cr√©√©\n"
     ]
    }
   ],
   "source": [
    "def create_engineered_features(df):\n",
    "    \"\"\"Cr√©er des features engineered pour r√©duire l'overfitting\"\"\"\n",
    "    df_eng = df.copy()\n",
    "    \n",
    "    print(\"\\n=== FEATURE ENGINEERING ===\")\n",
    "    \n",
    "    # Age de la voiture (plus informatif que l'ann√©e seule)\n",
    "    if 'Year' in df_eng.columns:\n",
    "        current_year = 2024  # Ajustez selon vos donn√©es\n",
    "        df_eng['Car_Age'] = current_year - df_eng['Year']\n",
    "        print(\"‚úÖ Car_Age cr√©√©\")\n",
    "    \n",
    "    # Ratio de d√©pr√©ciation\n",
    "    if 'Present_Price' in df_eng.columns and 'Selling_Price' in df_eng.columns:\n",
    "        df_eng['Depreciation_Ratio'] = (df_eng['Present_Price'] - df_eng['Selling_Price']) / df_eng['Present_Price']\n",
    "        df_eng['Depreciation_Ratio'] = df_eng['Depreciation_Ratio'].clip(0, 1)  # Limiter entre 0 et 1\n",
    "        print(\"‚úÖ Depreciation_Ratio cr√©√©\")\n",
    "    \n",
    "    # Binning de l'√¢ge en cat√©gories\n",
    "    if 'Car_Age' in df_eng.columns:\n",
    "        df_eng['Age_Category'] = pd.cut(df_eng['Car_Age'], \n",
    "                                       bins=[0, 2, 5, 10, float('inf')], \n",
    "                                       labels=['Nouveau', 'Recent', 'Moyen', 'Ancien'])\n",
    "        print(\"‚úÖ Age_Category cr√©√©\")\n",
    "    \n",
    "    # Log transformation pour les prix (r√©duire la variance)\n",
    "    price_columns = ['Selling_Price', 'Present_Price']\n",
    "    for col in price_columns:\n",
    "        if col in df_eng.columns:\n",
    "            df_eng[f'{col}_log'] = np.log1p(df_eng[col])\n",
    "            print(f\"‚úÖ {col}_log cr√©√©\")\n",
    "    \n",
    "    return df_eng\n",
    "\n",
    "df_clean = create_engineered_features(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a80337-68cc-456b-8fd3-8fedc80f6bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. D√âTECTION ET SUPPRESSION DES DOUBLONS SOPHISTIQU√âS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "137a8ec4-41c9-4ad7-9812-ad28f011b750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SUPPRESSION DOUBLONS SOPHISTIQU√âS ===\n",
      "Doublons exacts supprim√©s: 1\n",
      "Doublons par colonnes cl√©s supprim√©s: 44\n"
     ]
    }
   ],
   "source": [
    "def remove_sophisticated_duplicates(df):\n",
    "    \"\"\"Supprimer les doublons sophistiqu√©s\"\"\"\n",
    "    df_dedup = df.copy()\n",
    "    \n",
    "    print(\"\\n=== SUPPRESSION DOUBLONS SOPHISTIQU√âS ===\")\n",
    "    \n",
    "    # Doublons exacts\n",
    "    exact_dups = df_dedup.duplicated().sum()\n",
    "    df_dedup = df_dedup.drop_duplicates()\n",
    "    print(f\"Doublons exacts supprim√©s: {exact_dups}\")\n",
    "    \n",
    "    # Doublons bas√©s sur des colonnes cl√©s (exemple)\n",
    "    key_columns = ['Year', 'Present_Price', 'Fuel_Type', 'Transmission']\n",
    "    if all(col in df_dedup.columns for col in key_columns):\n",
    "        before_count = len(df_dedup)\n",
    "        df_dedup = df_dedup.drop_duplicates(subset=key_columns)\n",
    "        key_dups = before_count - len(df_dedup)\n",
    "        print(f\"Doublons par colonnes cl√©s supprim√©s: {key_dups}\")\n",
    "    \n",
    "    return df_dedup\n",
    "\n",
    "df_clean = remove_sophisticated_duplicates(df_clean)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ec4c39d-a859-4c01-b266-2cb11eed60f1",
   "metadata": {},
   "source": [
    "6. VALIDATION ET FILTRAGE DE COH√âRENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b67e69d-67e6-4427-ba42-aed8d898faab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VALIDATION DE COH√âRENCE ===\n",
      "Lignes avec prix de vente > 120% prix pr√©sent supprim√©es: 0\n",
      "Voitures avec ann√©e future supprim√©es: 0\n",
      "Lignes avec Driven_kms n√©gatif supprim√©es: 0\n",
      "Total lignes supprim√©es pour incoh√©rence: 0\n"
     ]
    }
   ],
   "source": [
    "def validate_data_consistency(df):\n",
    "    \"\"\"Valider la coh√©rence des donn√©es\"\"\"\n",
    "    df_valid = df.copy()\n",
    "    \n",
    "    print(\"\\n=== VALIDATION DE COH√âRENCE ===\")\n",
    "    \n",
    "    initial_count = len(df_valid)\n",
    "    \n",
    "    # Selling_Price ne peut pas √™tre sup√©rieur √† Present_Price (dans la plupart des cas)\n",
    "    if 'Selling_Price' in df_valid.columns and 'Present_Price' in df_valid.columns:\n",
    "        inconsistent = df_valid['Selling_Price'] > df_valid['Present_Price'] * 1.2  # 20% de marge\n",
    "        df_valid = df_valid[~inconsistent]\n",
    "        print(f\"Lignes avec prix de vente > 120% prix pr√©sent supprim√©es: {inconsistent.sum()}\")\n",
    "    \n",
    "    # Ann√©es futures\n",
    "    if 'Year' in df_valid.columns:\n",
    "        future_years = df_valid['Year'] > 2024\n",
    "        df_valid = df_valid[~future_years]\n",
    "        print(f\"Voitures avec ann√©e future supprim√©es: {future_years.sum()}\")\n",
    "    \n",
    "    # Kilom√©trage n√©gatif (si cette colonne existe)\n",
    "    km_columns = [col for col in df_valid.columns if 'km' in col.lower() or 'mileage' in col.lower()]\n",
    "    for col in km_columns:\n",
    "        negative_km = df_valid[col] < 0\n",
    "        df_valid = df_valid[~negative_km]\n",
    "        print(f\"Lignes avec {col} n√©gatif supprim√©es: {negative_km.sum()}\")\n",
    "    \n",
    "    final_count = len(df_valid)\n",
    "    print(f\"Total lignes supprim√©es pour incoh√©rence: {initial_count - final_count}\")\n",
    "    \n",
    "    return df_valid\n",
    "\n",
    "df_clean = validate_data_consistency(df_clean)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39d6dd06-145d-4869-b0e2-7c7802eee573",
   "metadata": {},
   "source": [
    "7. NETTOYAGE FINAL ET RAPPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ab0506c-2ef5-4e50-8181-b97978693d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RAPPORT DE NETTOYAGE FINAL\n",
      "============================================================\n",
      "Shape originale: (301, 9)\n",
      "Shape nettoy√©e: (223, 14)\n",
      "Lignes supprim√©es: 78 (25.9%)\n",
      "\n",
      "‚úÖ Aucune valeur manquante restante!\n",
      "\n",
      "Statistiques Selling_Price:\n",
      "  Moyenne: 3.90\n",
      "  M√©diane: 3.35\n",
      "  √âcart-type: 3.42\n",
      "  Min-Max: 0.10 - 18.00\n",
      "\n",
      "Features finales (14):\n",
      "  Age_Category: category (2 valeurs uniques)\n",
      "  Car_Age: int64 (13 valeurs uniques)\n",
      "  Car_Name: object (32 valeurs uniques)\n",
      "  Depreciation_Ratio: float64 (213 valeurs uniques)\n",
      "  Driven_kms: int64 (166 valeurs uniques)\n",
      "  Fuel_Type: object (3 valeurs uniques)\n",
      "  Owner: int64 (1 valeurs uniques)\n",
      "  Present_Price: float64 (135 valeurs uniques)\n",
      "  Present_Price_log: float64 (135 valeurs uniques)\n",
      "  Selling_Price: float64 (121 valeurs uniques)\n",
      "  Selling_Price_log: float64 (121 valeurs uniques)\n",
      "  Selling_type: object (2 valeurs uniques)\n",
      "  Transmission: object (2 valeurs uniques)\n",
      "  Year: int64 (13 valeurs uniques)\n"
     ]
    }
   ],
   "source": [
    "def final_cleanup_and_report(df_original, df_clean):\n",
    "    \"\"\"Nettoyage final et rapport de synth√®se\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RAPPORT DE NETTOYAGE FINAL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Shape originale: {df_original.shape}\")\n",
    "    print(f\"Shape nettoy√©e: {df_clean.shape}\")\n",
    "    print(f\"Lignes supprim√©es: {len(df_original) - len(df_clean)} ({(len(df_original) - len(df_clean))/len(df_original)*100:.1f}%)\")\n",
    "    \n",
    "    # Valeurs manquantes restantes\n",
    "    missing_final = df_clean.isnull().sum()\n",
    "    if missing_final.sum() > 0:\n",
    "        print(f\"\\nValeurs manquantes restantes:\")\n",
    "        for col, count in missing_final[missing_final > 0].items():\n",
    "            print(f\"  {col}: {count} ({count/len(df_clean)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ Aucune valeur manquante restante!\")\n",
    "    \n",
    "    # Distribution de la variable cible\n",
    "    if 'Selling_Price' in df_clean.columns:\n",
    "        target_stats = df_clean['Selling_Price'].describe()\n",
    "        print(f\"\\nStatistiques Selling_Price:\")\n",
    "        print(f\"  Moyenne: {target_stats['mean']:.2f}\")\n",
    "        print(f\"  M√©diane: {target_stats['50%']:.2f}\")\n",
    "        print(f\"  √âcart-type: {target_stats['std']:.2f}\")\n",
    "        print(f\"  Min-Max: {target_stats['min']:.2f} - {target_stats['max']:.2f}\")\n",
    "    \n",
    "    # Features finales\n",
    "    print(f\"\\nFeatures finales ({len(df_clean.columns)}):\")\n",
    "    for col in sorted(df_clean.columns):\n",
    "        dtype = df_clean[col].dtype\n",
    "        nunique = df_clean[col].nunique()\n",
    "        print(f\"  {col}: {dtype} ({nunique} valeurs uniques)\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "df_final = final_cleanup_and_report(df, df_clean)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61ea19e6-f3c5-41a1-8716-09e8b3f2cf5f",
   "metadata": {},
   "source": [
    "8. SAUVEGARDE DES DONN√âES NETTOY√âES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2836d23-981a-4591-8439-7c01a6e937d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Donn√©es nettoy√©es sauvegard√©es dans 'car_data_cleaned.csv'\n",
      "\n",
      "=== UTILISEZ MAINTENANT CES DONN√âES POUR L'ENTRA√éNEMENT ===\n",
      "# Pour charger les donn√©es nettoy√©es:\n",
      "df_clean = pd.read_csv('car_data_cleaned.csv')\n",
      "# Proc√©dez avec votre pipeline de ML habituel\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarder les donn√©es nettoy√©es\n",
    "df_final.to_csv('car_data_cleaned.csv', index=False)\n",
    "print(f\"\\n‚úÖ Donn√©es nettoy√©es sauvegard√©es dans 'car_data_cleaned.csv'\")\n",
    "\n",
    "# Cr√©er un rapport de nettoyage\n",
    "cleaning_report = {\n",
    "    'original_shape': df.shape,\n",
    "    'final_shape': df_final.shape,\n",
    "    'rows_removed': len(df) - len(df_final),\n",
    "    'percentage_removed': (len(df) - len(df_final)) / len(df) * 100,\n",
    "    'columns_added': len(df_final.columns) - len(df.columns)\n",
    "}\n",
    "\n",
    "print(f\"\\n=== UTILISEZ MAINTENANT CES DONN√âES POUR L'ENTRA√éNEMENT ===\")\n",
    "print(\"# Pour charger les donn√©es nettoy√©es:\")\n",
    "print(\"df_clean = pd.read_csv('car_data_cleaned.csv')\")\n",
    "print(\"# Proc√©dez avec votre pipeline de ML habituel\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6dfbc512-7f73-4c8f-80e3-0166f5599b95",
   "metadata": {},
   "source": [
    "9. VISUALISATIONS OPTIONNELLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "993d67ae-d041-4da6-ba83-f4f14b3cb3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ NETTOYAGE TERMIN√â ! Vos donn√©es sont maintenant pr√™tes pour un entra√Ænement sans overfitting.\n"
     ]
    }
   ],
   "source": [
    "def create_cleaning_visualizations(df_original, df_clean):\n",
    "    \"\"\"Cr√©er des visualisations pour montrer l'impact du nettoyage\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Distribution du prix avant/apr√®s\n",
    "    if 'Selling_Price' in df_original.columns:\n",
    "        axes[0,0].hist(df_original['Selling_Price'], bins=50, alpha=0.7, label='Original', color='red')\n",
    "        axes[0,0].hist(df_clean['Selling_Price'], bins=50, alpha=0.7, label='Nettoy√©', color='blue')\n",
    "        axes[0,0].set_title('Distribution Selling_Price')\n",
    "        axes[0,0].legend()\n",
    "        axes[0,0].set_xlabel('Prix')\n",
    "        axes[0,0].set_ylabel('Fr√©quence')\n",
    "    \n",
    "    # Boxplot comparatif\n",
    "    if 'Selling_Price' in df_original.columns:\n",
    "        data_comparison = [df_original['Selling_Price'].dropna(), df_clean['Selling_Price'].dropna()]\n",
    "        axes[0,1].boxplot(data_comparison, labels=['Original', 'Nettoy√©'])\n",
    "        axes[0,1].set_title('Boxplot Selling_Price')\n",
    "        axes[0,1].set_ylabel('Prix')\n",
    "    \n",
    "    # Corr√©lations avant/apr√®s (si suffisamment de colonnes num√©riques)\n",
    "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 2:\n",
    "        corr_clean = df_clean[numeric_cols].corr()\n",
    "        im = axes[1,0].imshow(corr_clean, cmap='coolwarm', aspect='auto')\n",
    "        axes[1,0].set_title('Matrice de corr√©lation (donn√©es nettoy√©es)')\n",
    "        plt.colorbar(im, ax=axes[1,0])\n",
    "    \n",
    "    # Valeurs manquantes avant/apr√®s\n",
    "    missing_orig = df_original.isnull().sum()\n",
    "    missing_clean = df_clean.isnull().sum()\n",
    "    \n",
    "    x_pos = range(len(missing_orig))\n",
    "    axes[1,1].bar([x-0.2 for x in x_pos], missing_orig.values, width=0.4, label='Original', alpha=0.7)\n",
    "    axes[1,1].bar([x+0.2 for x in x_pos], missing_clean.values, width=0.4, label='Nettoy√©', alpha=0.7)\n",
    "    axes[1,1].set_title('Valeurs manquantes par colonne')\n",
    "    axes[1,1].set_xticks(x_pos)\n",
    "    axes[1,1].set_xticklabels(missing_orig.index, rotation=45)\n",
    "    axes[1,1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('data_cleaning_report.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìä Graphiques de nettoyage sauvegard√©s dans 'data_cleaning_report.png'\")\n",
    "\n",
    "# Cr√©er les visualisations (optionnel)\n",
    "# create_cleaning_visualizations(df, df_final)\n",
    "\n",
    "print(\"\\nüéâ NETTOYAGE TERMIN√â ! Vos donn√©es sont maintenant pr√™tes pour un entra√Ænement sans overfitting.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9bc07ccc-e25d-45bc-9696-2352e5b94ddd",
   "metadata": {},
   "source": [
    "Remark:\n",
    "Given that the dataset contains only 301 samples and we have engineered 3 additional features, there is a significant risk of overfitting. With a relatively high feature-to-sample ratio, the model might capture noise and specific patterns in the training data that do not generalize well to unseen data. To mitigate this risk, it is crucial to perform proper feature selection, apply regularization techniques, and use rigorous cross-validation strategies to ensure the model‚Äôs robustness and generalization capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f9e71-7878-47a7-9755-b81c309d9a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
